{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor, Future, as_completed\n",
    "import tempfile\n",
    "\n",
    "from rich import print\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser, PSSyntaxError, PSEOF\n",
    "from pdfminer.pdfdocument import PDFDocument, PDFEncryptionError, PDFNoValidXRef\n",
    "\n",
    "import pikepdf\n",
    "from pikepdf._core import PdfError\n",
    "\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "import sh\n",
    "from sh import pdftitle\n",
    "from sh import pdfsandwich\n",
    "\n",
    "from numpy import array\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "books_directory = Path(Path.home(), \"Books\")\n",
    "damaged_directory = Path(Path.home(), \"damaged_pdfs\")\n",
    "books = list(books_directory.glob('*.pdf'))\n",
    "\n",
    "title_lengths = []\n",
    "\n",
    "analysis_metadata_boilerplate = {\n",
    "        \"Publisher\": None,\n",
    "        \"Producer\": None,\n",
    "        \"CreationDate\": None,\n",
    "        \"ModDate\": None,\n",
    "        \"PublicationDate\": None,\n",
    "        \"Comments\": None,\n",
    "        \"Edition\": None,\n",
    "        \"ISBN\": None,\n",
    "        \"Topics\": None,\n",
    "        \"Author\": None,\n",
    "        \"Title\": None,\n",
    "        \"pdftitle_title_original\": None,\n",
    "        \"pdftitle_title_max2\": None,\n",
    "        \"pdftitle_title_eliot\": None,\n",
    "        \"Filename\": None,\n",
    "        \"PageCount\": None,\n",
    "        \"AbsCogEase\": None,\n",
    "        \"RelCogEase\": None,\n",
    "        \"Read\": False,\n",
    "        \"RetentionScore\": None,\n",
    "        \"RetentionPressure\": None,\n",
    "        \"FleschReadingEaseScore\": None,\n",
    "        \"FleschKincaidReadabilityTest\": None,\n",
    "        \"GunningFog\": None,\n",
    "        \"SMOGgrade\": None, # Simple Measure of Gobbledygook\n",
    "        \"LinsearWriteFormula\": None,\n",
    "        \"ColemanLiauIndex\": None,\n",
    "        \"AutomatedReadabilityIndex\": None,\n",
    "        \"LasbarhetsIndex\": None,\n",
    "        \"DaleChallFormula\": None,\n",
    "        \"NewDaleChallFormula\": None,\n",
    "        \"FryReadabilityGraph\": None,\n",
    "        \"FORCASTFormula\": None,\n",
    "        \"GolubSyntacticDensityScore\": None,\n",
    "        \"ClozeDeletionTest\": None,\n",
    "        \"LixReadabilityFormula\": None,\n",
    "        \"BormuthReadabilityIndex\": None,\n",
    "        \"PowersSumnerKearlReadabilityFormula\": None,\n",
    "        \"RaygorEstimateGraph\": None,\n",
    "        \"RaygorReadabilityFormula\": None,\n",
    "        \"SPACHEReadabilityFormula\": None,\n",
    "        \"LexileFramework\": None,\n",
    "        \"ATOSReadabilityFormula\": None,\n",
    "        \"CohMetrixPsycholinguisticsMeasurements\": None,\n",
    "    }\n",
    "\n",
    "def decode_str(string: str) -> str:\n",
    "    try:\n",
    "        return UnicodeDammit(string).unicode_markup\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def decrypt_pdf(pdf_path: Path):\n",
    "    with pikepdf.open(pdf_path, password=\"\", allow_overwriting_input=True) as pdf:\n",
    "        pdf.save(pdf_path)\n",
    "\n",
    "def get_pdf_version(pdf_path: Path) -> str:\n",
    "    with pikepdf.open(pdf_path) as pdf:\n",
    "        return pdf.pdf_version\n",
    "\n",
    "def normalize_xmp_types(pdf_path: Path) -> None:\n",
    "    with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n",
    "        with pdf.open_metadata() as meta:\n",
    "            null_set = {None}\n",
    "\n",
    "            if meta.get(\"dc:creator\") == null_set:\n",
    "                meta.update({\"dc:creator\": [\"\"]})\n",
    "\n",
    "        pdf.save()\n",
    "\n",
    "def initialize_analysis_json(pdf_path: Path, pdf_metadata: dict, overwrite: bool=False) -> None:\n",
    "    with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n",
    "        with pdf.open_metadata() as meta:\n",
    "            if not meta.get(\"pdf:Json\") or overwrite:\n",
    "                analysis_metadata_boilerplate.update(pdf_metadata)\n",
    "                json_meta = json.dumps(analysis_metadata_boilerplate)\n",
    "                meta.update({\"pdf:Json\": json_meta})\n",
    "        pdf.save()\n",
    "\n",
    "def get_analysis_dict(pdf_path: Path) -> dict:\n",
    "    try:\n",
    "        with pikepdf.open(pdf_path) as pdf:\n",
    "            with pdf.open_metadata() as meta:\n",
    "                analysis_json = meta.get(\"pdf:Json\")\n",
    "                \n",
    "                return json.loads(analysis_json)\n",
    "            \n",
    "    except PdfError:\n",
    "        pdf_path.rename(Path(damaged_directory, pdf_path.name))\n",
    "        return None\n",
    "\n",
    "def update_analysis_json(pdf_path: Path, analysis_dict: dict):\n",
    "    dict_meta = {}\n",
    "    with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n",
    "        with pdf.open_metadata() as meta:\n",
    "            dict_meta = get_analysis_dict(pdf_path)\n",
    "\n",
    "            if not dict_meta:\n",
    "                return None\n",
    "\n",
    "            dict_meta.update(analysis_dict)\n",
    "            json_meta = json.dumps(dict_meta)\n",
    "\n",
    "            meta.update({\"pdf:Json\": json_meta})\n",
    "        pdf.save()\n",
    "    return dict_meta if dict_meta else None\n",
    "\n",
    "def scrape_pdf_metadata(pdf_path: Path, parser: str) -> dict:\n",
    "    pdf_metadata = {}\n",
    "\n",
    "    match parser:\n",
    "        case \"pdfminer\":\n",
    "            with pdf_path.open(mode=\"rb\") as pdf:\n",
    "                try:\n",
    "                    parse = PDFParser(pdf)\n",
    "                    doc = PDFDocument(parse)\n",
    "\n",
    "                    if len(doc.info) == 1:\n",
    "                        info = doc.info[0]\n",
    "\n",
    "                        title = None\n",
    "                        if decode_str(info.get('Title')) and len(decode_str(info.get('Title'))) < 1000:\n",
    "                            title = decode_str(info.get('Title'))\n",
    "\n",
    "                        pdf_metadata |= {\n",
    "                            \"Publisher\": decode_str(info.get('Publisher')),\n",
    "                            \"Producer\": decode_str(info.get('Producer')),\n",
    "                            \"CreationDate\": decode_str(info.get('CreationDate')),\n",
    "                            \"ModDate\": decode_str(info.get('ModDate')),\n",
    "                            \"PublicationDate\": decode_str(info.get('PublicationDate')),\n",
    "                            \"Comments\": decode_str(info.get('Comments')),\n",
    "                            \"Edition\": decode_str(info.get('Edition')),\n",
    "                            \"ISBN\": decode_str(info.get('ISBN')),\n",
    "                            \"Topics\": decode_str(info.get('Topics')),\n",
    "                            \"Author\": decode_str(info.get('Author')),\n",
    "                            \"Title\":  title,\n",
    "                            \"Filename\": pdf_path.name,\n",
    "                            \"PageCount\": decode_str(info.get('PageCount')),\n",
    "                            \"PdfVersion\": get_pdf_version(pdf_path)\n",
    "                        }\n",
    "\n",
    "                    normalize_xmp_types(pdf_path)\n",
    "                    initialize_analysis_json(pdf_path, pdf_metadata, True)\n",
    "                    \n",
    "                except PDFEncryptionError:\n",
    "                    decrypt_pdf(pdf_path)\n",
    "                    scrape_pdf_metadata(pdf_path, parser)\n",
    "                    return\n",
    "                except PSSyntaxError:\n",
    "                    decrypt_pdf(pdf_path)\n",
    "                    scrape_pdf_metadata(pdf_path, parser)\n",
    "                    return\n",
    "                except PSEOF:\n",
    "                    # repair\n",
    "                    return\n",
    "                except PDFNoValidXRef:\n",
    "                    # repair\n",
    "                    return\n",
    "                \n",
    "\n",
    "    return pdf_metadata\n",
    "\n",
    "def get_pdftitle_titles(pdf_path: Path, analysis_dict: dict):\n",
    "    page_number = 0\n",
    "\n",
    "    def get_original(pdf_path: Path, page_number: int):\n",
    "        return (\"pdftitle_title_original\", pdftitle(pdf=pdf_path, title_case=True, page_number=page_number))\n",
    "    \n",
    "    def get_max2(pdf_path: Path, page_number: int):\n",
    "        return (\"pdftitle_title_max2\", pdftitle(pdf=pdf_path, algo=\"max2\", title_case=True, page_number=page_number))\n",
    "\n",
    "    def get_eliot(pdf_path: Path, page_number: int):\n",
    "        return (\"pdftitle_title_eliot\", pdftitle(pdf=pdf_path, algo=\"eliot\", title_case=True, page_number=page_number))\n",
    "    \n",
    "    def ocr_pages(pdf_path: Path, start: int, stop: int):\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "            source = pikepdf.Pdf.open(pdf_path, allow_overwriting_input=True)\n",
    "\n",
    "            sliced_pdf = pikepdf.Pdf.new()\n",
    "            temp_pdf_path = Path(tmpdirname, \"temp.pdf\")\n",
    "            temp_ocr_path = Path(tmpdirname, \"temp_ocr.pdf\")\n",
    "            \n",
    "            pages = source.pages[start:stop]\n",
    "            sliced_pdf.pages.extend(pages)\n",
    "            sliced_pdf.save(temp_pdf_path)\n",
    "\n",
    "            pdfsandwich(\"-lang\", \"eng+equ+osd\", temp_pdf_path)\n",
    "            temp_ocr_path.rename(temp_pdf_path)\n",
    "            pdfsandwich(\"-lang\", \"eng+equ+osd\", temp_pdf_path)\n",
    "\n",
    "            ocr_pdf = pikepdf.Pdf.open(temp_ocr_path)\n",
    "            for i, page in enumerate(ocr_pdf.pages):\n",
    "                source.pages.append(page)\n",
    "                source.pages[i].emplace(source.pages[-1])\n",
    "                del source.pages[-1]\n",
    "\n",
    "            source.save()\n",
    "\n",
    "    def run_pdftitle_algo(func, pdf_path: Path, ocr_already_attempted: bool = False):\n",
    "        page_scan_count = 5\n",
    "        retries = 5\n",
    "        attempts = 0\n",
    "\n",
    "        for page_number in range(0, page_scan_count):\n",
    "            try:\n",
    "                title: tuple = func(pdf_path, page_number)\n",
    "\n",
    "                if len(title[1]) > 1000:\n",
    "                    raise OverflowError\n",
    "\n",
    "                title_lengths.append(len(title[1]))\n",
    "                analysis_dict.update({title[0]: title[1]})\n",
    "                break\n",
    "            except sh.ErrorReturnCode:\n",
    "                attempts += 1\n",
    "                continue\n",
    "            except OverflowError:\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "        if attempts == retries and not ocr_already_attempted:\n",
    "            ocr_pages(pdf_path, start=0, stop=page_scan_count)\n",
    "            run_pdftitle_algo(func, pdf_path, True)\n",
    "\n",
    "    run_pdftitle_algo(get_original, pdf_path)\n",
    "    run_pdftitle_algo(get_max2, pdf_path)\n",
    "    run_pdftitle_algo(get_eliot, pdf_path)\n",
    "\n",
    "    update_analysis_json(pdf_path, analysis_dict)\n",
    "    return analysis_dict\n",
    "\n",
    "def scrape(book):\n",
    "    # print(\"+\" * 30, \"\\n\", book.name, \"\\n\", \"+\" * 30)\n",
    "\n",
    "    scrape_pdf_metadata(book, \"pdfminer\")\n",
    "    analysis_dict = get_analysis_dict(book)\n",
    "\n",
    "    if not analysis_dict:\n",
    "        return None\n",
    "    \n",
    "    analysis_dict = get_pdftitle_titles(book, analysis_dict)\n",
    "\n",
    "    if analysis_dict[\"Title\"] and len(analysis_dict[\"Title\"]) < 1000:\n",
    "        title_lengths.append(len(analysis_dict[\"Title\"]))\n",
    "\n",
    "def run() -> None:\n",
    "\n",
    "    # book = \"A Systematic Comparison of Various Statistical Alignment Models.pdf\"\n",
    "    # pdf_path = Path(Path.home(), \"Books\", book)\n",
    "\n",
    "    # scrape(pdf_path)\n",
    "\n",
    "    for book in books:\n",
    "        scrape(book)\n",
    "\n",
    "    lengths = array(title_lengths)\n",
    "\n",
    "    sns.histplot(lengths)\n",
    "\n",
    "    print(lengths)\n",
    "\n",
    "    # with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    #     futures = []\n",
    "\n",
    "    #     for book in books:\n",
    "    #         future = executor.submit(scrape, book)\n",
    "    #         futures.append(future)\n",
    "\n",
    "    #     for future in as_completed(futures):\n",
    "    #         futures.remove(future)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
